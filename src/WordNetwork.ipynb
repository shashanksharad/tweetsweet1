{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordNetwork.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY-L1lvlcUTX"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from collections import Counter\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "glove = torchtext.vocab.GloVe(name=\"6B\", # trained on Wikipedia 2014 corpus of 6 billion words\n",
        "dim=50) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLXmV5D0W9fL",
        "outputId": "6d7fd3a3-840b-4c13-c734-9ba59b31dc36"
      },
      "source": [
        "def word_vec(word):\n",
        "  return glove[word]\n",
        "\n",
        "def cosine_sim(vector_1, vector_2):\n",
        "  return torch.cosine_similarity(vector_1.unsqueeze(0), vector_2.unsqueeze(0))\n",
        "\n",
        "\n",
        "def simmilarity(word1, word2):\n",
        "  return cosine_sim(word_vec(word1), word_vec(word2))\n",
        "\n",
        "simmilarity('nigga', 'nigro')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1597])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49zchVD9d-h5",
        "outputId": "bfc7fd5c-a300-45d1-a3af-a7cfdd7b9901"
      },
      "source": [
        "word_vec(\"shit\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-6.1396e-01,  1.7478e-01,  5.7008e-02, -1.6326e+00,  1.5258e-01,\n",
              "        -7.5744e-01,  1.7240e-01, -3.0148e-01,  2.7009e-02,  1.6019e+00,\n",
              "        -7.0764e-01,  6.8023e-01,  2.2553e-01,  1.0254e+00,  1.6152e-01,\n",
              "         1.4885e-01,  8.8233e-01, -8.6374e-03, -1.4107e-03,  1.1392e+00,\n",
              "        -7.0934e-01, -3.1530e-01,  1.6758e+00,  4.1512e-01,  6.7164e-01,\n",
              "        -6.3516e-01, -1.5862e+00,  8.7716e-01,  1.0955e+00, -9.2124e-01,\n",
              "         3.5458e-01,  1.2302e-01,  1.2504e-02,  1.6347e+00, -2.8170e-02,\n",
              "        -5.4797e-02,  6.9939e-01, -7.9258e-02,  1.0559e-01,  2.1636e-01,\n",
              "        -3.5064e-01, -1.9988e-01, -8.4036e-01,  6.9721e-01,  6.6648e-01,\n",
              "        -2.0986e-01, -6.5187e-02, -1.5728e-01,  4.3393e-01,  3.0487e-01])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieUIUBS9le_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c25b0f2-b6a4-483a-faee-7e2df0a2dd5a"
      },
      "source": [
        "tweets = ['@444keni 😂😂😂😂😂', 'Just say Rip to that man and keep it moving', 'The hate Virgil got was just niggas being weird an…pinions, you can’t please everybody and that’s ok', 'This year is draining all of us hoping for the best https://t.co/IwDk4ql0RH', 'Chukwu chebe anyi niile', 'Virgil showed young niggas allover the world this shit possible Rest In Peace', 'VIRGIL IS DEAD WHAT IN THE FUCK ??', 'If you extra ugly I’ll give you 600 on god', 'Sending all ugly mfs 200 dollars where y’all at', '@notaprilfr I swear I do LMAOOO', 'RT @notaprilfr: @DAMAGEDTROOP410 we need u to say you love violence and confusion again', '@doogiehowsergte Nigga what you do 😂😂 ??', 'I can’t live foreverrrrr I know someday I gotta le…inging his heart out lmao https://t.co/N7YrlOSfOg', 'It’s called Don’t matter https://t.co/7g2dDEYCbD', 'That bar was random as shit but bro obviously had to get it off his chest lmao', 'When youngboy said “My lil cousin they African so …t me with that racist shit” I felt that he get it', 'I love my life so much that I don’t want any fuck niggas in it', '@CyberRuntz It’s life changing fr', 'RT @CyberRuntz: @DAMAGEDTROOP410 put me on to fufu and my life been different ever since 🤝', '@natenumbaeight LMAOOO', '@kimmythegod LMAOOO', 'RT @flvmeprincesss: in the words of @DAMAGEDTROOP410  these pussy niggas not feelin me', 'Nigga said he be putting acid in his dog treats lmaooo', 'RT @NicoStillSFS: This the funniest space ever bruh @DAMAGEDTROOP410', 'Get on this bitch and whisper https://t.co/zGWI6h5l3t', 'Imagine still recovering from genocide that happen…just to see a whole other race claiming to be you', 'I know indigenous people be so upset when they see niggas calling themselves native Americans', '@promise_akubue You speak Igbo lmao ?', 'When I’m dead my niggas gotta say stupid shit like this about me https://t.co/KiPOk4xKE5', 'I kinda like this nigga https://t.co/EVSv3CVcYl', 'You can’t please everybody fuck these niggas', 'I feel like a new man it’s still hope for me', 'I really went to sleep at 9PM for the 1st time in like 13 years and actually stayed sleep', 'Before you speak on a nigga you don’t know ask yourself “is dick riding this important to me”', 'RT @tayyv: this the realest nigga on twitter 😭😭😭😭😭', 'RT @chrollo_lord: When that jerk sauce hit the the back of ya throat https://t.co/iDfY0TwbOi', 'Jamaicans cooking Mac and cheese everybody RUN!!', 'She on her own time 😂😂 https://t.co/vsPVgT01dG', '@anadeldank 😂😂😂 she not rushing for nobody', 'RT @anadeldank: Im your mom 😂😂😂', 'My mother just putting the turkey in at 4PM LMAO she ghetto as shit', 'LMAOOOOOO https://t.co/WGvfLPW116', 'What white people be eating for thanksgiving, green eggs and ham ??', 'RIP my father and free my brother', 'Nigga got seaweed on his plate https://t.co/FAwTPxbwRL', 'RT @chuychvzz: realest shit ive heard all year', 'So thankful I’m nothing like these niggas', 'Fuck thanksgiving 💯', 'A Puerto Rican Muslim is insane', 'RT @luvsxka: This man @DAMAGEDTROOP410 be talkin his shit but people don’t wanna listen', 'I didn’t know niggas could get this stupid until I started getting on Twitter', '@caassshhhhh I thought you was from Egypt', 'I was 6 blunts in fighting for my life ngl https://t.co/UaD4wDPujg', 'RT @staxhigh_swiggs: @stormiji only @DAMAGEDTROOP4…being the voice of reason https://t.co/MwrIecvCc0', '@slaymsav That conversation was making my brain rot oh my god lmaooo', 'Ima keep talking my shit I fear https://t.co/2VOWlsKIDv', 'RT @lajjsabat: I know what your saying please keep speaking bro @DAMAGEDTROOP410', 'But I get it tho common sense isn’t common and nig…hate themselves so shit will never be that simple', 'White people fucked us up so bad as a whole niggas…h different experiences, It’s just that simple...', 'I’m surprised these niggas not pregnant smh https://t.co/W2DLuaeTqw', 'RT @444guap: niggas on my dick at 3 am, crazy - @DAMAGEDTROOP410', 'RT @buwi6: @DAMAGEDTROOP410 pls ily keep going 💀', 'I literally run a business I know where I’m going …\\uDE02 https://t.co/1Fm4OM4UUz https://t.co/u2geMpDCNI', 'Anti black girl from the suburbs you sound like a white woman LMAO https://t.co/ePLy8rLOcW', '- A nigga that think Africa a country LMAO https://t.co/uQCLxoNo6F', '@gtedoogie Nigga that space was insane😂', '@slaymsav 😂😂😂😂😂', '@slaymsav What happen I just woke up', 'You Scottish niggas insane LMAOOO https://t.co/9qilh0e6dC', 'RT @notscoeuros: @DAMAGEDTROOP410 is cool as fuck', 'Yo wtf going on LMAOOO  https://t.co/V3TzfNH9Ue', 'Yo wtf going on LMAOOOO', 'LMAOOO https://t.co/siaq4yb0q1', '@XAVALEXANDER I might gotta do that', 'RT @XAVALEXANDER: make that 757 popup event happen', 'Who got some cows and pigs for sale tap in let’s get rich', 'About to go to Virginia for no reason', 'Bitch got the name Larry tatted on her face 😂😂😂😂', 'She just took this dick wayyy back I’m buying a saxophone tomorrow I fear', 'Just got some head to some Jazz music she sucked that dick back into the 60’s', 'During the transatlantic slave trade enslaved Afri… white niggas was hating hard when they seen that', 'She don’t deserve kids I hate this bitch', 'And the dumb bitch in the back comparing skin tone… I’m going back to Africa https://t.co/O7IuG2qiNo', 'Nigga 15 https://t.co/K8pFKYMa56', 'RT @StonedDaniel: I wonder - Kanye West', 'You ever bump a song that make you feel good to be alive', 'He said “Damn my nigga” https://t.co/SOKmApT4px', 'Had the gun to that nigga chest and farted https://t.co/YY56Fqmcoh', '@419kiddd Lmaooo', 'Shit was so intense', 'I remember in 2015 I was hitting a lick and I farted', '@natenumbaeight You just like me', 'You better get the fuck outta my face', 'Wish I would let a bitch with some cards tell me about myself', '@valoravee LMAOOOOO', '@RIPSCHICAGO Niggas be Delusional LMAOOO', 'Shordy said niggas wouldn’t survive in the UK bitch Ima Nigerian from Baltimore LMAOO']\n",
        "\n",
        "tx_scores = [0, 0.134, 1, 0.001, 0.025, 1, 0.999, 0.96, 1, 0.003, 0.011, 0.998, 0.999, 0, 1, 1, 1, 0, 0.18, 0, 0, 1, 0.999, 0, 1, 0.546, 0.999, 0.021, 1, 0.999, 1, 0, 0, 1, 0.999, 0.959, 0.001, 0, 0.005, 0.039, 1, 0.001, 0.002, 0.001, 0.934, 1, 1, 1, 0.999, 1, 1, 0, 0.386, 0, 0.767, 1, 0, 1, 1, 1, 1, 0, 0, 0.99, 1, 0.999, 0, 0, 1, 1, 0.013, 0.019, 0, 0, 0, 0.001, 0, 1, 1, 0.993, 1, 1, 1, 0.959, 0, 0.001, 1, 1, 0, 0.062, 0.001, 0, 1, 1, 0, 0.998, 1]\n",
        "tx_class = [1 if tx_scores[i]>=0.5 else 0 for i in range(len(tx_scores))]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXI5sgFNfMB8",
        "outputId": "a3923ff8-84a8-4254-ec7f-6da60819fe3b"
      },
      "source": [
        "def deEmojify(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',text)\n",
        "\n",
        "\n",
        "\n",
        "all_words_tx = []\n",
        "all_words_ntx = []\n",
        "for cls, sen in zip(tx_class, tweets):\n",
        "  # print(cls)\n",
        "  # print(sen)\n",
        "  sen = re.sub(r\"http\\S+\", \"\", sen)\n",
        "  sen = deEmojify(sen)\n",
        "  # print(sen)\n",
        "  tokens = word_tokenize(sen)\n",
        "  # convert to lower case\n",
        "  tokens = [w.lower() for w in tokens]\n",
        "  # remove punctuation from each word\n",
        " \n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  stripped = [w.translate(table) for w in tokens]\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  words = [word for word in stripped if word.isalpha()]\n",
        "  # filter out stop words\n",
        "  \n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  if cls==1:\n",
        "    all_words_tx += [w for w in words if not w in stop_words and not w in ['rt'] and w in glove.itos]\n",
        "  else:\n",
        "    all_words_ntx += [w for w in words if not w in stop_words and not w in ['rt'] and w in glove.itos]\n",
        "\n",
        "\n",
        "n_words = 30 #total number of words to be shown in the graph\n",
        "n_tx_words = len(set(all_words_tx))\n",
        "n_ntx_words = len(set(all_words_ntx))\n",
        "\n",
        "n_top_tx_words = 0 #number of words to be shown from toxic class\n",
        "n_top_ntx_words = 0 #number of words to be shown from nontoxic class\n",
        "\n",
        "if n_tx_words < n_words*0.5 and n_ntx_words > n_words*0.5:\n",
        "  n_top_tx_words = n_tx_words\n",
        "  n_top_ntx_words = int(n_words-n_tx_words)\n",
        "\n",
        "if n_tx_words > n_words*0.5 and n_ntx_words < n_words*0.5:\n",
        "  n_top_ntx_words = n_ntx_words\n",
        "  n_top_tx_words = int(n_words-n_ntx_words)\n",
        "\n",
        "if n_tx_words < n_words*0.5 and n_ntx_words < n_words*0.5:\n",
        "  n_top_ntx_words = n_ntx_words\n",
        "  n_top_tx_words = n_tx_words\n",
        "\n",
        "if n_tx_words > n_words*0.5 and n_ntx_words > n_words*0.5:\n",
        "  n_top_ntx_words = int(0.5*n_words)\n",
        "  n_top_tx_words = int(0.5*n_words)\n",
        "\n",
        "print(n_top_ntx_words, n_top_tx_words)\n",
        "\n",
        "\n",
        "tx_words, ntx_words = Counter(all_words_tx).most_common(n_top_tx_words), Counter(all_words_ntx).most_common(n_top_ntx_words)\n",
        "tx_words = [t[0] for t in tx_words]\n",
        "ntx_words = [t[0] for t in ntx_words]\n",
        "print(tx_words)\n",
        "print(ntx_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "15 15\n",
            "['niggas', 'nigga', 'shit', 'got', 'fuck', 'get', 'bitch', 'back', 'know', 'said', 'like', 'dick', 'virgil', 'people', 'insane']\n",
            "['going', 'keep', 'life', 'ever', 'like', 'say', 'rip', 'man', 'feel', 'sleep', 'time', 'run', 'reason', 'know', 'happen']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CCHEYiPhu9D"
      },
      "source": [
        "all_words = list(set(tx_words+ntx_words))\n",
        "sim = np.zeros((len(all_words), len(all_words)))\n",
        "for i, w in enumerate(all_words):\n",
        "  for j, w_ in enumerate(all_words):\n",
        "    sim[i, j] = simmilarity(w, w_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWIX3JUEm5CV"
      },
      "source": [
        "def make_set(lst_tpls):\n",
        "  d0, d1 = {item[0] for item in lst_tpls}, {item[1] for item in lst_tpls}\n",
        "  d_all = d1.union(d0)\n",
        "  return list(d_all)\n",
        "\n",
        "thrsh = 0.5\n",
        "l = list(zip(list(np.where(sim>thrsh)[0]), list(np.where(sim>thrsh)[1]))) #list of word index tuples\n",
        "l = list({tuple(sorted(item)) for item in l}) #remove reversed duplicates e.g. (1,2) and (2,1)\n",
        "l = [(l_[0], l_[1]) for l_ in l if l_[0]!=l_[1]]\n",
        "\n",
        "uniq_wrd_ids = make_set(l)\n",
        "uniq_wrds = [all_words[uniq_wrd_ids[i]] for i in range(len(uniq_wrd_ids))]\n",
        "df = pd.DataFrame(uniq_wrds, columns = [\"word\"])\n",
        "df.to_csv('all_words.csv', index = False)\n",
        "\n",
        "sim_wrds = [[all_words[l[i][0]], all_words[l[i][1]], sim[l[i][0], l[i][1]]] for i in range(len(l))]\n",
        "df = pd.DataFrame(sim_wrds, columns=[\"source\", \"target\", \"similarity\"])\n",
        "df.head(100)\n",
        "df.to_csv('word_nerwork.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3150n1qEM-zD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}